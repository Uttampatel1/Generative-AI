{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07","metadata":{}},{"cell_type":"markdown","source":"## 1. Install required libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:22:05.317510Z","iopub.execute_input":"2024-03-04T06:22:05.317771Z","iopub.status.idle":"2024-03-04T06:23:02.702297Z","shell.execute_reply.started":"2024-03-04T06:22:05.317748Z","shell.execute_reply":"2024-03-04T06:23:02.701371Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.0 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:23:09.918692Z","iopub.execute_input":"2024-03-04T06:23:09.919072Z","iopub.status.idle":"2024-03-04T06:24:17.815715Z","shell.execute_reply.started":"2024-03-04T06:23:09.919040Z","shell.execute_reply":"2024-03-04T06:24:17.814797Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-04 06:23:19.320544: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-04 06:23:19.320636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-04 06:23:19.460339: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Token:  ·····································\nAdd token as git credential? (Y/n)  n\n"},{"name":"stdout","text":"Token is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:25:34.687374Z","iopub.execute_input":"2024-03-04T06:25:34.688224Z","iopub.status.idle":"2024-03-04T06:25:34.692141Z","shell.execute_reply.started":"2024-03-04T06:25:34.688191Z","shell.execute_reply":"2024-03-04T06:25:34.691188Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:48:10.790287Z","iopub.execute_input":"2024-03-04T10:48:10.791130Z","iopub.status.idle":"2024-03-04T10:48:10.796552Z","shell.execute_reply.started":"2024-03-04T10:48:10.791090Z","shell.execute_reply":"2024-03-04T10:48:10.795664Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### 2. Loading dataset","metadata":{}},{"cell_type":"code","source":"huggingface_dataset_name = \"neil-code/dialogsum-test\"\ndataset = load_dataset(huggingface_dataset_name)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:45:46.473142Z","iopub.execute_input":"2024-03-04T06:45:46.473495Z","iopub.status.idle":"2024-03-04T06:45:49.239565Z","shell.execute_reply.started":"2024-03-04T06:45:46.473467Z","shell.execute_reply":"2024-03-04T06:45:49.238777Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"866ba14cde5547999ac5783813fc9875"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 1.81M/1.81M [00:00<00:00, 13.3MB/s]\nDownloading data: 100%|██████████| 441k/441k [00:00<00:00, 4.32MB/s]\nDownloading data: 100%|██████████| 447k/447k [00:00<00:00, 3.95MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1b329e719ef4531bc3dd1f77887cac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ab99bdf8b62400f81f7c008a6dd44e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3fff73045d141558eae01831cde118a"}},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:46:35.283441Z","iopub.execute_input":"2024-03-04T06:46:35.284161Z","iopub.status.idle":"2024-03-04T06:46:35.290783Z","shell.execute_reply.started":"2024-03-04T06:46:35.284125Z","shell.execute_reply":"2024-03-04T06:46:35.289824Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3. Create Bitsandbytes configuration","metadata":{}},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:48:16.646218Z","iopub.execute_input":"2024-03-04T06:48:16.646575Z","iopub.status.idle":"2024-03-04T06:48:16.653281Z","shell.execute_reply.started":"2024-03-04T06:48:16.646547Z","shell.execute_reply":"2024-03-04T06:48:16.652412Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 4. Loading the Pre-Trained model","metadata":{}},{"cell_type":"code","source":"model_name='microsoft/phi-2'\ndevice_map = {\"\": 0}\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:48:50.052147Z","iopub.execute_input":"2024-03-04T06:48:50.052571Z","iopub.status.idle":"2024-03-04T06:49:18.277176Z","shell.execute_reply.started":"2024-03-04T06:48:50.052535Z","shell.execute_reply":"2024-03-04T06:49:18.276442Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"645e3be36a1441df842ee93dbe8b5a0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"045a6b8428cd4071b580a1b01596b34a"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n- configuration_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fffaded6e4c143f58b5f0c8b12739b45"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n- modeling_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d2050e92ec74826952f350c772e39ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb764ef618814913980573a4dec3c4c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a7585c7a5884852b08859d3ad8fb7ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17af91e6f1054487a63a9b08c43ae05d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1157f359c19416795c1f9b17134ca23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c830bfd4f4e413c8c6bdfd9931008b9"}},"metadata":{}}]},{"cell_type":"markdown","source":"### 5. Tokenization","metadata":{}},{"cell_type":"code","source":"# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:50:31.156969Z","iopub.execute_input":"2024-03-04T06:50:31.157344Z","iopub.status.idle":"2024-03-04T06:50:32.320597Z","shell.execute_reply.started":"2024-03-04T06:50:31.157314Z","shell.execute_reply":"2024-03-04T06:50:32.319691Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b5f1b6210342b29c5cba0b24d7506b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51d272c4e81f4550b7946dd9f2af6361"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1070b88ee67f4619b7eb7590284de0ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e5903787f34755ae8207203672a1b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8115fc6a82424db7a528e9c5824cb228"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa6dc730a9148f48ad3f88487fae01c"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:48:27.404436Z","iopub.execute_input":"2024-03-04T10:48:27.405423Z","iopub.status.idle":"2024-03-04T10:48:27.410278Z","shell.execute_reply.started":"2024-03-04T10:48:27.405392Z","shell.execute_reply":"2024-03-04T10:48:27.409283Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"GPU memory occupied: 6823 MB.\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model,p, maxlen=100, sample=True):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:48:28.907978Z","iopub.execute_input":"2024-03-04T10:48:28.908560Z","iopub.status.idle":"2024-03-04T10:48:29.064719Z","shell.execute_reply.started":"2024-03-04T10:48:28.908526Z","shell.execute_reply":"2024-03-04T10:48:29.063816Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 6. Test the Model with Zero Shot Inferencing","metadata":{}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n# res = gen(original_model,formatted_prompt,100,)\n# #print(res[0])\n# output = res[0].split('Output:\\n')[1]\n\nres = original_model.generate(input_ids=tokenizer.encode(formatted_prompt, return_tensors=\"pt\"), \n                              max_new_tokens=100)\noutput = tokenizer.decode(res[0], skip_special_tokens=True)\n\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:54:10.001348Z","iopub.execute_input":"2024-03-04T06:54:10.002210Z","iopub.status.idle":"2024-03-04T06:54:14.617869Z","shell.execute_reply.started":"2024-03-04T06:54:10.002177Z","shell.execute_reply":"2024-03-04T06:54:14.616963Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\nPerson1 and Person2 are at a party, and Person1 asks if they can have a dance. Person2 agrees and compliments Person1 on their appearance. Person1 thanks them and expresses their happiness with the party. Person2 agrees that it's a great party and suggests having a drink to celebrate.\n\nCPU times: user 4.24 s, sys: 134 ms, total: 4.38 s\nWall time: 4.61 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 7. Pre-processing dataset","metadata":{}},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:56:39.597241Z","iopub.execute_input":"2024-03-04T06:56:39.598112Z","iopub.status.idle":"2024-03-04T06:56:39.604892Z","shell.execute_reply.started":"2024-03-04T06:56:39.598078Z","shell.execute_reply":"2024-03-04T06:56:39.603865Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:56:58.174114Z","iopub.execute_input":"2024-03-04T06:56:58.174950Z","iopub.status.idle":"2024-03-04T06:56:58.184108Z","shell.execute_reply.started":"2024-03-04T06:56:58.174918Z","shell.execute_reply":"2024-03-04T06:56:58.183122Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:48:36.001431Z","iopub.execute_input":"2024-03-04T10:48:36.001801Z","iopub.status.idle":"2024-03-04T10:48:36.006887Z","shell.execute_reply.started":"2024-03-04T10:48:36.001769Z","shell.execute_reply":"2024-03-04T10:48:36.005962Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"GPU memory occupied: 6823 MB.\n","output_type":"stream"}]},{"cell_type":"code","source":"## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T06:57:11.824122Z","iopub.execute_input":"2024-03-04T06:57:11.824475Z","iopub.status.idle":"2024-03-04T06:57:24.646843Z","shell.execute_reply.started":"2024-03-04T06:57:11.824445Z","shell.execute_reply":"2024-03-04T06:57:24.646067Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Found max lenth: 2048\n2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d460fc8b90214fdbb9459ea3a83ac930"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f6ba860da6487fbb2e0d528a6fa5a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4056735973142db9a34aad6c2e2e02f"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f6d039e9712439dad5a874eb218cb83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65d12da85a144f0fa8a93e5aa374bfc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4185966375461cb6aedf4ebe2e54dd"}},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:48:38.645236Z","iopub.execute_input":"2024-03-04T10:48:38.645596Z","iopub.status.idle":"2024-03-04T10:48:38.650868Z","shell.execute_reply.started":"2024-03-04T10:48:38.645566Z","shell.execute_reply":"2024-03-04T10:48:38.649864Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (1999, 3)\nValidation: (499, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1999\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 8. Preparing the model for QLoRA\n\nNow, let's perform Parameter Efficient Fine-Tuning (PEFT) fine-tuning. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning. PEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, in essence, enables efficient model fine-tuning using fewer computational resources, often achievable with just a single GPU. Following LoRA fine-tuning for a specific task or use case, the outcome is an unchanged original LLM and the emergence of a considerably smaller \"LoRA adapter,\" often representing a single-digit percentage of the original LLM size (in MBs rather than GBs).\n\nDuring inference, the LoRA adapter must be combined with its original LLM. The advantage lies in the ability of many LoRA adapters to reuse the original LLM, thereby reducing overall memory requirements when handling multiple tasks and use cases.\n\nNote the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained. r is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n\nalpha is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations.","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:02:52.152125Z","iopub.execute_input":"2024-03-04T07:02:52.152847Z","iopub.status.idle":"2024-03-04T07:02:52.157030Z","shell.execute_reply.started":"2024-03-04T07:02:52.152815Z","shell.execute_reply":"2024-03-04T07:02:52.156015Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# 2 - Using the prepare_model_for_kbit_training method from PEFT\n# Preparing the Model for QLoRA\noriginal_model = prepare_model_for_kbit_training(original_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:02:55.159700Z","iopub.execute_input":"2024-03-04T07:02:55.160552Z","iopub.status.idle":"2024-03-04T07:02:55.177721Z","shell.execute_reply.started":"2024-03-04T07:02:55.160518Z","shell.execute_reply":"2024-03-04T07:02:55.176712Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### 10. Setup PEFT for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training \n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\npeft_model = get_peft_model(original_model, config)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:05:04.573081Z","iopub.execute_input":"2024-03-04T07:05:04.573938Z","iopub.status.idle":"2024-03-04T07:05:04.955218Z","shell.execute_reply.started":"2024-03-04T07:05:04.573894Z","shell.execute_reply":"2024-03-04T07:05:04.954240Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"original_model","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:03:58.951076Z","iopub.execute_input":"2024-03-04T07:03:58.951417Z","iopub.status.idle":"2024-03-04T07:03:58.968827Z","shell.execute_reply.started":"2024-03-04T07:03:58.951393Z","shell.execute_reply":"2024-03-04T07:03:58.967955Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"PhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2560, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=2560, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2560, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=2560, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2560, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=2560, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (dense): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2560, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=2560, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n          )\n          (rotary_emb): PhiRotaryEmbedding()\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# See how the model looks different now, with the LoRA adapters added:\nprint(peft_model)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:48:49.498786Z","iopub.execute_input":"2024-03-04T10:48:49.499129Z","iopub.status.idle":"2024-03-04T10:48:49.514320Z","shell.execute_reply.started":"2024-03-04T10:48:49.499104Z","shell.execute_reply":"2024-03-04T10:48:49.513338Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PhiForCausalLM(\n      (model): PhiModel(\n        (embed_tokens): Embedding(51200, 2560)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-31): 32 x PhiDecoderLayer(\n            (self_attn): PhiAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (dense): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): PhiRotaryEmbedding()\n            )\n            (mlp): PhiMLP(\n              (activation_fn): NewGELUActivation()\n              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n            )\n            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    all_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    percentage_trainable = (trainable_params / all_params) * 100 if all_params != 0 else 0\n    \n    print(\"Trainable Model Parameters:\", trainable_params)\n    print(\"All Model Parameters:\", all_params)\n    print(\"Percentage of Trainable Model Parameters:\", percentage_trainable, \"%\")\n\n# Example usage:\nprint_number_of_trainable_model_parameters(peft_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:09:43.830113Z","iopub.execute_input":"2024-03-04T07:09:43.830473Z","iopub.status.idle":"2024-03-04T07:09:43.850733Z","shell.execute_reply.started":"2024-03-04T07:09:43.830446Z","shell.execute_reply":"2024-03-04T07:09:43.849707Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Trainable Model Parameters: 20971520\nAll Model Parameters: 1542364160\nPercentage of Trainable Model Parameters: 1.3596996444730665 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 9. Train PEFT Adapter","metadata":{}},{"cell_type":"code","source":"output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=1000,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:10:26.896934Z","iopub.execute_input":"2024-03-04T07:10:26.897313Z","iopub.status.idle":"2024-03-04T07:10:26.912588Z","shell.execute_reply.started":"2024-03-04T07:10:26.897283Z","shell.execute_reply":"2024-03-04T07:10:26.911707Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"peft_training_args.device","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:48:53.540779Z","iopub.execute_input":"2024-03-04T10:48:53.541448Z","iopub.status.idle":"2024-03-04T10:48:53.546951Z","shell.execute_reply.started":"2024-03-04T10:48:53.541415Z","shell.execute_reply":"2024-03-04T10:48:53.546054Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:10:46.790359Z","iopub.execute_input":"2024-03-04T07:10:46.790974Z","iopub.status.idle":"2024-03-04T10:42:23.860021Z","shell.execute_reply.started":"2024-03-04T07:10:46.790941Z","shell.execute_reply":"2024-03-04T10:42:23.859057Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 3:31:25, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.651600</td>\n      <td>1.392267</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.187600</td>\n      <td>1.382473</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.445700</td>\n      <td>1.353482</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.205100</td>\n      <td>1.365688</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.439200</td>\n      <td>1.343503</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.138300</td>\n      <td>1.360301</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.402300</td>\n      <td>1.339185</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.146200</td>\n      <td>1.343461</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.443500</td>\n      <td>1.333517</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.223800</td>\n      <td>1.338064</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.454600</td>\n      <td>1.333477</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.166300</td>\n      <td>1.338689</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.431600</td>\n      <td>1.330305</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.201600</td>\n      <td>1.329263</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.397000</td>\n      <td>1.327706</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.182900</td>\n      <td>1.329974</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.438800</td>\n      <td>1.325447</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.199000</td>\n      <td>1.325331</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.449100</td>\n      <td>1.323952</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.188100</td>\n      <td>1.326594</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.407900</td>\n      <td>1.322894</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.143400</td>\n      <td>1.324479</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>1.344300</td>\n      <td>1.321965</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.185100</td>\n      <td>1.323482</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>1.369300</td>\n      <td>1.321438</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.116300</td>\n      <td>1.322328</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>1.374400</td>\n      <td>1.320415</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.127500</td>\n      <td>1.322280</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>1.419300</td>\n      <td>1.320155</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.157200</td>\n      <td>1.319156</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>1.434600</td>\n      <td>1.318845</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.136600</td>\n      <td>1.319255</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>1.398400</td>\n      <td>1.318890</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.126600</td>\n      <td>1.319468</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>1.451900</td>\n      <td>1.317826</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.141800</td>\n      <td>1.317755</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>1.364400</td>\n      <td>1.317268</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.144000</td>\n      <td>1.317385</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>1.338800</td>\n      <td>1.317214</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.110300</td>\n      <td>1.317248</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=1.292110050201416, metrics={'train_runtime': 12695.6305, 'train_samples_per_second': 0.315, 'train_steps_per_second': 0.079, 'total_flos': 1.850993530739712e+16, 'train_loss': 1.292110050201416, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:49:02.048423Z","iopub.execute_input":"2024-03-04T10:49:02.049225Z","iopub.status.idle":"2024-03-04T10:49:02.053861Z","shell.execute_reply.started":"2024-03-04T10:49:02.049195Z","shell.execute_reply":"2024-03-04T10:49:02.052954Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"GPU memory occupied: 6823 MB.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 10. Evaluate the Model Qualitatively (Human Evaluation)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id = \"microsoft/phi-2\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:42:23.861801Z","iopub.execute_input":"2024-03-04T10:42:23.862728Z","iopub.status.idle":"2024-03-04T10:42:27.729340Z","shell.execute_reply.started":"2024-03-04T10:42:23.862692Z","shell.execute_reply":"2024-03-04T10:42:27.728576Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3f134d649eb4a62b905729ec4cfa452"}},"metadata":{}}]},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:42:27.730459Z","iopub.execute_input":"2024-03-04T10:42:27.730742Z","iopub.status.idle":"2024-03-04T10:42:27.865571Z","shell.execute_reply.started":"2024-03-04T10:42:27.730717Z","shell.execute_reply":"2024-03-04T10:42:27.864655Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training-1709536226/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:47:39.731228Z","iopub.execute_input":"2024-03-04T10:47:39.732013Z","iopub.status.idle":"2024-03-04T10:47:40.283722Z","shell.execute_reply.started":"2024-03-04T10:47:39.731980Z","shell.execute_reply":"2024-03-04T10:47:40.282955Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 5\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\n# peft_model_res = gen(ft_model,prompt,100,)\n# peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n\npeft_model_res = ft_model.generate(input_ids=tokenizer.encode(prompt, return_tensors=\"pt\"), \n                              max_new_tokens=100)\npeft_model_output = tokenizer.decode(peft_model_res[0], skip_special_tokens=True)\n\n\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('###')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:47:47.911440Z","iopub.execute_input":"2024-03-04T10:47:47.912078Z","iopub.status.idle":"2024-03-04T10:47:56.810954Z","shell.execute_reply.started":"2024-03-04T10:47:47.912045Z","shell.execute_reply":"2024-03-04T10:47:56.810029Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: You're finally here! What took so long?\n#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n#Person2#: I don't think it can be avoided, to be honest.\n#Person1#: perhaps it would be better if you started taking public transport system to work.\n#Person2#: I think it's something that I'll have to consider. The public transport system is pretty good.\n#Person1#: It would be better for the environment, too.\n#Person2#: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n#Person1#: Taking the subway would be a lot less stressful than driving as well.\n#Person2#: The only problem is that I'm going to really miss having the freedom that you have with a car.\n#Person1#: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n#Person2#: That's true. I could certainly use the exercise!\n#Person1#: So, are you going to quit driving to work then?\n#Person2#: Yes, it's not good for me or for the environment.\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\nInstruct: Summarize the following conversation.\n#Person1#: You're finally here! What took so long?\n#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n#Person2#: I don't think it can be avoided, to be honest.\n#Person1#: perhaps it would be better if you started taking public transport system to work.\n#Person2#: I think it's something that I'll have to consider. The public transport system is pretty good.\n#Person1#: It would be better for the environment, too.\n#Person2#: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n#Person1#: Taking the subway would be a lot less stressful than driving as well.\n#Person2#: The only problem is that I'm going to really miss having the freedom that you have with a car.\n#Person1#: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n#Person2#: That's true. I could certainly use the exercise!\n#Person1#: So, are you going to quit driving to work then?\n#Person2#: Yes, it's not good for me or for the environment.\nOutput:\n#Person2# got stuck in traffic again and #Person1# suggests #Person2# take public transport system to work. #Person2# agrees and will quit driving to work.\n\n\nCPU times: user 8.83 s, sys: 59.5 ms, total: 8.89 s\nWall time: 8.89 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 11. Evaluate the Model Quantitatively (with ROUGE Metric)","metadata":{}},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:48:00.856387Z","iopub.execute_input":"2024-03-04T10:48:00.857238Z","iopub.status.idle":"2024-03-04T10:48:04.482674Z","shell.execute_reply.started":"2024-03-04T10:48:00.857204Z","shell.execute_reply":"2024-03-04T10:48:04.481947Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62fb662b9c5c4d50a0e5f5298db18246"}},"metadata":{}}]},{"cell_type":"code","source":"# from datasets import load_dataset\n# from transformers import (\n#     AutoModelForCausalLM,\n#     AutoTokenizer,\n#     BitsAndBytesConfig,\n#     HfArgumentParser,\n#     TrainingArguments,\n#     GenerationConfig,\n#     set_seed\n# )\n# from tqdm import tqdm\n# from trl import SFTTrainer\n# import torch\n# import pandas as pd\n\n# def gen(model, prompt, max_length):\n#     set_seed(42)  # Set seed for reproducibility\n#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n#     outputs = model.generate(input_ids=input_ids, max_length=max_length)\n#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:49:10.019458Z","iopub.execute_input":"2024-03-04T10:49:10.020279Z","iopub.status.idle":"2024-03-04T10:49:10.024583Z","shell.execute_reply.started":"2024-03-04T10:49:10.020248Z","shell.execute_reply":"2024-03-04T10:49:10.023511Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:49:13.659588Z","iopub.execute_input":"2024-03-04T10:49:13.660407Z","iopub.status.idle":"2024-03-04T10:51:14.677550Z","shell.execute_reply.started":"2024-03-04T10:49:13.660373Z","shell.execute_reply":"2024-03-04T10:51:14.676586Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  Person 1: Ms. Dawson, I need you to take a dic...   \n1  Person 1: Ms. Dawson, I need you to take a dic...   \n2  Person 1: Ms. Dawson, I need you to take a dic...   \n3  Person2: I'm going to stop driving to work bec...   \n4  Person2: I'm going to stop driving to work bec...   \n5  Person1 and Person2 discuss the traffic conges...   \n6  Kate informed that Masha and Hero are getting ...   \n7  Person1 informs Person2 that Masha and Hero ar...   \n8  Kate informed that Masha and Hero are getting ...   \n9  Person1 and Person2 are at a party, and Person...   \n\n                                peft_model_summaries  \n0  #Person1# asks Ms. Dawson to take a dictation ...  \n1  #Person1# asks Ms. Dawson to take a dictation ...  \n2  #Person1# asks Ms. Dawson to take a dictation ...  \n3  #Person2# got stuck in traffic again and #Pers...  \n4  #Person2# got stuck in traffic again and #Pers...  \n5  #Person2# got stuck in traffic again and #Pers...  \n6  Masha and Hero are getting divorced. Masha tel...  \n7  Masha and Hero are getting divorced. Masha tel...  \n8  Masha and Hero are getting divorced. Masha tel...  \n9  Brian's birthday is celebrated by #Person1#. #...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>Person 1: Ms. Dawson, I need you to take a dic...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>Person2: I'm going to stop driving to work bec...</td>\n      <td>#Person2# got stuck in traffic again and #Pers...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>Person2: I'm going to stop driving to work bec...</td>\n      <td>#Person2# got stuck in traffic again and #Pers...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>Person1 and Person2 discuss the traffic conges...</td>\n      <td>#Person2# got stuck in traffic again and #Pers...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>Kate informed that Masha and Hero are getting ...</td>\n      <td>Masha and Hero are getting divorced. Masha tel...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>Person1 informs Person2 that Masha and Hero ar...</td>\n      <td>Masha and Hero are getting divorced. Masha tel...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>Kate informed that Masha and Hero are getting ...</td>\n      <td>Masha and Hero are getting divorced. Masha tel...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>Person1 and Person2 are at a party, and Person...</td>\n      <td>Brian's birthday is celebrated by #Person1#. #...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:51:50.568821Z","iopub.execute_input":"2024-03-04T10:51:50.569707Z","iopub.status.idle":"2024-03-04T10:52:03.684674Z","shell.execute_reply.started":"2024-03-04T10:51:50.569672Z","shell.execute_reply":"2024-03-04T10:52:03.683672Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:52:05.799481Z","iopub.execute_input":"2024-03-04T10:52:05.799847Z","iopub.status.idle":"2024-03-04T10:52:08.383405Z","shell.execute_reply.started":"2024-03-04T10:52:05.799819Z","shell.execute_reply":"2024-03-04T10:52:08.382444Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65986023affe44a7bf608cc3a98879ab"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.31883399650409144, 'rouge2': 0.1141601784769837, 'rougeL': 0.2504126220395657, 'rougeLsum': 0.2617197796662095}\nPEFT MODEL:\n{'rouge1': 0.33803151823591016, 'rouge2': 0.11258304158031926, 'rougeL': 0.223457162563649, 'rougeLsum': 0.23682561957755804}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:52:23.413747Z","iopub.execute_input":"2024-03-04T10:52:23.414612Z","iopub.status.idle":"2024-03-04T10:52:23.420418Z","shell.execute_reply.started":"2024-03-04T10:52:23.414580Z","shell.execute_reply":"2024-03-04T10:52:23.419492Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 1.92%\nrouge2: -0.16%\nrougeL: -2.70%\nrougeLsum: -2.49%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}